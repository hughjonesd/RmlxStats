[{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":null,"dir":"","previous_headings":"","what":"Repository Guidelines","title":"Repository Guidelines","text":"project builds Rmlx GPU math package expose statistical workflows. Many conventions mirror upstream repository; deviations RmlxStats-specific guidance called .","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"project-structure--module-organization","dir":"","previous_headings":"","what":"Project Structure & Module Organization","title":"Repository Guidelines","text":"R/ holds exported R wrappers, S3 methods, roxygen docs. Mirror upstream layout like mlxs-lm.R adding public APIs (use mlxs_ prefix). tests/testthat/ groups unit specs domain (test-mlxs-lm.R, test-pca.R); add new files test-feature.R. Vignettes (planned) live vignettes/; update user-facing features land. DESCRIPTION, NAMESPACE, AGENTS.md manage package metadata coordination docs.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"build-test-and-development-commands","dir":"","previous_headings":"","what":"Build, Test, and Development Commands","title":"Repository Guidelines","text":"R -q -e 'devtools::document()' rebuilds NAMESPACE Rd files roxygen comments. R -q -e 'devtools::build()' creates source tarball; R -q -e 'devtools::check()' runs formal package checks. R -q -e 'devtools::test()' runs testthat suite; use R -q -e 'devtools::load_all()' rapid iteration. Rcpp glue added, also run R -q -e 'Rcpp::compileAttributes()' regenerate RcppExports.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"coding-style--naming-conventions","dir":"","previous_headings":"","what":"Coding Style & Naming Conventions","title":"Repository Guidelines","text":"Use two-space indents R; keep lines 100 characters match upstream style. Public functions use mlxs_ prefix (mlxs_lm, mlxs_glm). S3 methods follow generic.class (print.mlxs_lm). Document R functions roxygen #' blocks; let @export drive NAMESPACE entries. Prefer snake_case internal helpers (as_model_matrix).","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"testing-guidelines","dir":"","previous_headings":"","what":"Testing Guidelines","title":"Repository Guidelines","text":"Write tests testthat tests/testthat; keep scenarios focused readable. Use CPU-friendly fixtures (small matrices) GPU CPU paths run quickly. Run R -q -e 'devtools::test()' locally; machines failures acceptable MLX absent, prefer explicit skips informative messages GPU required. Within workspace can assume MLX Rmlx installed working; add skip_if_not_available scaffolding around MLX usage unless directed otherwise.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"working-style-expectations","dir":"","previous_headings":"","what":"Working Style Expectations","title":"Repository Guidelines","text":"KEEP SIMPLE. Prefer direct expression idea elaborate helper stacks. single call (e.g., as_mlx()) communicates intent, use instead wrapping logic multiple conditionals. READ MLX LOCAL CODEBASE. changing behaviour, scan existing MLX helpers repo stay aligned current conventions—assume answer probably already exists somewhere nearby.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"commit--pull-request-guidelines","dir":"","previous_headings":"","what":"Commit & Pull Request Guidelines","title":"Repository Guidelines","text":"Follow imperative, capitalized commit messages (e.g., Add mlxs_lm interface). Document API changes summarize GPU/CPU paths covered PR descriptions. opening PR, run document(), test(), check(); include notable performance notes GPU speedups claimed.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"current-agent-notes-2025-10-31","dir":"","previous_headings":"","what":"Current Agent Notes (2025-10-31)","title":"Repository Guidelines","text":"MLX tensor creation Metal-backed work succeeds permissive sandbox (e.g., danger-full-access). Restricted sandboxes may block Metal device initialisation processx/callr usage. restricted modes can still run document(), test() (CPU paths), check(), GPU calls may throw c++ exception (unknown reason). Keep workspace tidy checks: remove temporary *.tar.gz artifacts .Rcheck/ directories created manual workflows.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"mlx-first-data-handling","dir":"","previous_headings":"Additional Guidance","what":"MLX-first Data Handling","title":"Repository Guidelines","text":"package exists validate mlx-based statistics workflow. Move inputs MLX arrays immediately keep long feasible; prefer MLX outputs anything feed computations. Base R representations acceptable user-facing summaries (printing, glance/tidy outputs, etc.) required R generics. constructing MLX data raw R vectors/scalars, favor explicit constructors Rmlx::mlx_scalar() / Rmlx::mlx_vector() / Rmlx::mlx_array() rather as_mlx() future readers can see intent call site.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"issue-tracking","dir":"","previous_headings":"Additional Guidance","what":"Issue Tracking","title":"Repository Guidelines","text":"Log backlog ideas directly GitHub issues (use gh issue create ...) instead keeping local docs/github-issues.md scratchpad. Include issue number PR summaries traceability. Residual bootstraps mlxs_glm supported (quasi)gaussian families—fail fast anything else requested rather silently downgrading case resampling. Keep bootstrap implementations MLX-native end--end: gather samples via MLX subsetting, refit mlxs_lm_fit / .mlxs_glm_fit_core, return MLX standard-error columns (intermediate .matrix()/stats::sd hops). mutating MLX arrays via [<-, convert update base matrix first needed avoid .vector.mlx warnings (e.g., large active-set updates mlxs_glmnet).","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"integration-with-rmlx","dir":"","previous_headings":"Additional Guidance","what":"Integration with Rmlx","title":"Repository Guidelines","text":"Always import Rmlx helpers (as_mlx, mlx_matmul, qr.mlx, etc.) via qualified namespace (Rmlx::). Update DESCRIPTION Imports/Remotes needed. Confirm function names latest pkgdown docs: https://hughjonesd.github.io/Rmlx/reference/index.html. R arrays column-major MLX tensors row-major; double-check axis handling reductions behave unexpectedly. Use explicit Rmlx::colMeans() Rmlx::colSums() rather unqualified calls avoid namespace confusion base R Rmlx methods.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"testing-notes","dir":"","previous_headings":"Additional Guidance","what":"Testing Notes","title":"Repository Guidelines","text":"Tests compare MLX-backed results base-R equivalents (e.g., stats::lm). Use expect_equal(..., tolerance = 1e-6) asserting floating point equality. iterate single spec: R -q -e 'devtools::test_file(\"tests/testthat/test-mlxs-lm.R\")'.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"documentation-workflow","dir":"","previous_headings":"Additional Guidance","what":"Documentation Workflow","title":"Repository Guidelines","text":"Roxygen comments power man/ docs; regenerate devtools::document() editing. Favor markdown lists/tables roxygen \\item. Update pkgdown site configuration (future work) new exports appear reference index.","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"what-mlx-does-well","dir":"","previous_headings":"Additional Guidance > MLX Performance Characteristics (Learned from mlxs_glmnet optimization)","what":"What MLX Does Well","title":"Repository Guidelines","text":"Parallel matrix operations: Large matrix multiplications, crossprod, etc. Batch processing: Operating multiple data elements simultaneously Staying device: Keeping data MLX throughout computation pipeline Vectorized operations: Element-wise operations large arrays","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"what-mlx-does-poorly-vs-fortranc","dir":"","previous_headings":"Additional Guidance > MLX Performance Characteristics (Learned from mlxs_glmnet optimization)","what":"What MLX Does Poorly (vs Fortran/C++)","title":"Repository Guidelines","text":"Small sequential operations: Building computation graphs overhead Frequent MLX↔︎R conversions: conversion triggers evaluation Branching logic: Conditionals early exits tight loops Single-element comparisons: .logical(x > tol) scalars overhead","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"conversion-overhead-is-usually-small","dir":"","previous_headings":"Additional Guidance > MLX Performance Characteristics (Learned from mlxs_glmnet optimization)","what":"Conversion Overhead Is Usually Small","title":"Repository Guidelines","text":"Investigation mlxs_glmnet revealed: - Eliminated ~20,000-50,000 conversions per run - Performance impact: ~0% (within measurement noise) - Algorithm choice matters far conversion count","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"when-to-convert-to-mlx","dir":"","previous_headings":"Additional Guidance > MLX Performance Characteristics (Learned from mlxs_glmnet optimization)","what":"When to Convert to MLX","title":"Repository Guidelines","text":"Early conversion wins: Convert inputs MLX immediately validation Late conversion R: convert final results back R Keep intermediate results MLX: Even need one value R, keep MLX version around subsequent computations","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"mlx-axis-handling","dir":"","previous_headings":"Additional Guidance > MLX Performance Characteristics (Learned from mlxs_glmnet optimization)","what":"MLX Axis Handling","title":"Repository Guidelines","text":"MLX uses C-style axis numbering underlying C++, Rmlx adapts R conventions Test axis operations carefully; sometimes best compute R convert result","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"comparison-operators-in-conditionals","dir":"","previous_headings":"Additional Guidance > MLX Performance Characteristics (Learned from mlxs_glmnet optimization)","what":"Comparison Operators in Conditionals","title":"Repository Guidelines","text":"MLX comparison returns MLX array: x > tol returns mlx object use directly (): must convert .logical() .numeric() Example: (.logical(delta < tol)) works; (delta < tol) errors","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"mlx_compile-capabilities-and-limitations","dir":"","previous_headings":"Additional Guidance > MLX Performance Characteristics (Learned from mlxs_glmnet optimization)","what":"mlx_compile() Capabilities and Limitations","title":"Repository Guidelines","text":"CAN compiled: - Pure MLX operations: matrix multiply, element-wise ops, reductions - Conditional logic using mlx_where() instead /else - Simple mathematical expressions (1/(1 + exp(-x)), etc.) - Functions returning single MLX array Measured speedups: - Simple iteration logic: 1.5-1.6x speedup - Complex compiled functions: potentially higher - Worth implementing hot paths use: - Inner loops executing 100s-1000s times - Pure computational logic without side effects - profiling shows function bottleneck Example pattern:","code":"inner_core <- function(x, y, params) {   # Pure MLX computation   result <- ... # expensive operations   result }  inner_compiled <- mlx_compile(inner_core)  for (iter in 1:max_iter) {   result <- inner_compiled(x, y, params)   # Convergence check in R   if (converged(result)) break }"},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"algorithm-design-for-gpu","dir":"","previous_headings":"Additional Guidance > MLX Performance Characteristics (Learned from mlxs_glmnet optimization)","what":"Algorithm Design for GPU","title":"Repository Guidelines","text":"Think parallel: Can multiple coordinates/lambdas processed simultaneously? Batch operations: Group similar operations together Avoid sequential updates: sequential step wastes GPU parallelism Example: Coordinate descent naturally parallelizes across coordinates; proximal gradient inherently sequential","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"storage-in-mlx","dir":"","previous_headings":"Additional Guidance > MLX Performance Characteristics (Learned from mlxs_glmnet optimization)","what":"Storage in MLX","title":"Repository Guidelines","text":"Keep result storage arrays (beta_store, intercept_store) MLX Assign directly MLX arrays: beta_store_mlx[, idx] <- beta_mlx convert entire result array end Eliminates per-iteration conversion overhead (though often small)","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"when-gpu-wont-help","dir":"","previous_headings":"Additional Guidance > MLX Performance Characteristics (Learned from mlxs_glmnet optimization)","what":"When GPU Won’t Help","title":"Repository Guidelines","text":"mlxs_glmnet optimization showed GPU provides speedup : 1. Algorithm inherently sequential (e.g., proximal gradient descent) 2. Operations small frequent (graph-building overhead dominates) 3. opportunity parallel processing 4. Highly optimized CPU code exists (e.g., Fortran glmnet) cases, focus algorithmic improvements GPU utilization.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/AGENTS.html","id":"handy-tips","dir":"","previous_headings":"Additional Guidance","what":"Handy Tips","title":"Repository Guidelines","text":"usethis:: helpers streamline chores; prefer package setup. MLX arrays currently lack default constructor; supply shape/dtype explicitly wiring future C++ glue. Discover available helpers library(help = \"Rmlx\") ls(envir = asNamespace(\"Rmlx\"), .names = TRUE). user-facing docs, prefer term array tensor match R conventions. Add concise internal comments non-obvious helper logic; keep codebase self-explanatory. Use mlx_zeros() pre-allocate result arrays rather converting R Profile optimizing: conversion overhead often smaller expected","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"executive-summary","dir":"","previous_headings":"","what":"Executive Summary","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Current mlxs_glmnet 10-154x slower glmnet::glmnet(), gap narrowing larger problems. main bottlenecks : Algorithm choice: Proximal gradient descent vs coordinate descent Convergence speed: iterations needed per lambda Memory transfers: Frequent R ↔︎ MLX conversions Limited GPU parallelism: Sequential coordinate updates","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"what-glmnet-does","dir":"","previous_headings":"Current Implementation Analysis","what":"What glmnet Does","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Algorithm: Cyclical coordinate descent soft thresholding Pathwise optimization: Warm starts previous lambda solutions Strong rules: Pre-screening reduce active set Implementation: Highly optimized Fortran Speed: 0.003-0.027s test problems","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"what-mlxs_glmnet-does","dir":"","previous_headings":"Current Implementation Analysis","what":"What mlxs_glmnet Does","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Algorithm: Proximal gradient descent active sets Pathwise optimization: ✓ (warm starts strong rules) Implementation: R + Rmlx (MLX tensors) Speed: 0.271-0.464s test problems Lines 144-180: Inner loop 4-6 MLX↔︎R conversions per iteration Lines 154, 171, 177, 180: .numeric() conversions Lines 210-220: Soft threshold creates 7 temporary MLX objects Line 62-64: Initial standardization","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"profiling-results","dir":"","previous_headings":"Current Implementation Analysis","what":"Profiling Results","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Individual operations (per call): - Matrix multiply: 0.24ms - Crossprod: 0.26ms - Soft threshold: 0.42ms - Link function: 0.01ms - MLX→R conversion: 0.04ms operations fast, ~400,000 conversions nlambda=100, maxit=1000, overhead adds .","code":"Problem Size     glmnet    mlxs_glmnet   Slowdown n=500,  p=50    0.003s     0.464s        154.7x n=2000, p=100   0.006s     0.253s        42.2x n=5000, p=200   0.027s     0.271s        10.0x"},{"path":[]},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"strategy-1-batched-coordinate-descent-radical","dir":"","previous_headings":"Optimization Strategies","what":"Strategy 1: Batched Coordinate Descent (RADICAL)","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Concept: Implement coordinate descent GPU using parallel batched updates. Approach: - Update multiple coordinates simultaneously parallel - Use asynchronous/randomized coordinate selection (research: TPA-SCD algorithm) - Batch process multiple lambda values - Keep computations MLX tensors final result Advantages: - Matches glmnet’s proven algorithm - Natural GPU parallelism (update p coordinates parallel) - Minimal R↔︎MLX transfers (start/end) - Can process multiple lambdas simultaneously Challenges: - Coordinate descent inherently sequential - Need careful handling shared memory - Requires asynchronous updates (may sacrifice convergence guarantees) - complex implementation Research Support: - “TPA-SCD can train SVM tera-scale dataset 1 minute 4 GPUs” - Parallel coordinate descent proven separable convex functions - Recent GPU implementations show 10-100x speedups Implementation Path: 1. Implement basic coordinate descent pure MLX (R conversions loop) 2. Add parallelization across coordinates (batched updates) 3. Add lambda-path batching (compute multiple lambdas together) 4. Optimize memory access patterns","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"strategy-2-optimized-proximal-gradient-incremental","dir":"","previous_headings":"Optimization Strategies","what":"Strategy 2: Optimized Proximal Gradient (INCREMENTAL)","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Concept: Keep current algorithm eliminate bottlenecks. Approach: - Remove inner-loop R↔︎MLX conversions - Use MLX-native convergence checks - Pre-compile inner loop mlx_compile() - Batch compute multiple lambda values - Simplify soft threshold use fewer temporaries Advantages: - Lower risk (incremental improvements) - Proven convergence properties - Easier maintain compatibility Challenges: - Still fundamentally slower convergence coordinate descent - May hit ceiling around 2-5x improvement Implementation Path: 1. Rewrite inner loop stay MLX (lines 140-175) 2. Use MLX-native max/convergence operations 3. Compile hot paths mlx_compile() 4. Profile iterate","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"strategy-3-hybrid-coordinate-proximal-balanced","dir":"","previous_headings":"Optimization Strategies","what":"Strategy 3: Hybrid Coordinate-Proximal (BALANCED)","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Concept: Combine best worlds. Approach: - Use coordinate descent smooth part - Use proximal operator penalty - Implement : θ_{j}^{k+1} = S_{λα}(θ_j^k - s∇_j f(θ^k)) - Keep computation MLX, update coordinates blocks Advantages: - Fast convergence coordinate descent - Simple proximal operator L1 penalty - Can parallelize within blocks - Proven work well elastic net Challenges: - complex pure approaches - Need tune block size Implementation Path: 1. Implement block coordinate descent structure 2. Integrate proximal gradient steps within blocks 3. Optimize block size GPU memory 4. Add strong rules active set management","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"strategy-4-direct-gpu-solver-alternative","dir":"","previous_headings":"Optimization Strategies","what":"Strategy 4: Direct GPU Solver (ALTERNATIVE)","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Concept: Use MLX’s built-optimization tools. Approach: - Formulate differentiable objective (smooth approximation L1) - Use MLX’s gradient computation optimizers - Implement proximal operator custom MLX operation - Leverage MLX’s compilation optimization Advantages: - Leverage MLX’s optimized infrastructure - Automatic differentiation - Potentially good compiler optimizations Challenges: - L1 penalty non-differentiable (need smoothing subgradient) - May match glmnet’s path-following behavior - Less control convergence","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"strategy-5-specialized-strong-rules-quick-win","dir":"","previous_headings":"Optimization Strategies","what":"Strategy 5: Specialized Strong Rules (QUICK WIN)","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Concept: Improve active set selection reduce computation. Approach: - aggressive strong rules screening - Dynamic active set sizing based GPU capacity - Early stopping active set stabilizes - KKT violation checks less frequently Advantages: - Can combine strategy - Low implementation risk - Proven effective glmnet Expected Impact: 20-40% speedup","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"phase-1-quick-wins-1-2-weeks","dir":"","previous_headings":"Recommended Approach","what":"Phase 1: Quick Wins (1-2 weeks)","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Implement Strategy 5 (better strong rules) Remove inner-loop conversions (Strategy 2, part 1) Expected: 2-3x speedup","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"phase-2-algorithm-change-2-4-weeks","dir":"","previous_headings":"Recommended Approach","what":"Phase 2: Algorithm Change (2-4 weeks)","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Implement Strategy 1 (batched coordinate descent) Strategy 3 (hybrid) Start single-threaded coordinate descent pure MLX Add parallelization incrementally Expected: 5-20x speedup (approaching glmnet speed)","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"phase-3-advanced-optimization-if-needed","dir":"","previous_headings":"Recommended Approach","what":"Phase 3: Advanced Optimization (if needed)","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Multi-lambda batching Compiled hot paths Optimized memory layout Expected: Additional 2-5x speedup","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"what-gpus-do-well","dir":"","previous_headings":"GPU-Specific Considerations","what":"What GPUs Do Well","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Parallel matrix operations Batch processing Vectorized operations High throughput saturated","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"what-gpus-do-poorly","dir":"","previous_headings":"GPU-Specific Considerations","what":"What GPUs Do Poorly","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Sequential operations Small operations high overhead Frequent CPU↔︎GPU transfers Branching conditionals","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"design-principles-for-gpu-implementation","dir":"","previous_headings":"GPU-Specific Considerations","what":"Design Principles for GPU Implementation","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Batch everything: Process multiple items together Stay device: Minimize CPU↔︎GPU transfers Saturate bandwidth: Use cores Coalesce memory: Access contiguous memory Avoid synchronization: Prefer asynchronous safe","code":""},{"path":[]},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"idea-1-multi-response-batching","dir":"","previous_headings":"Alternative Radical Ideas","what":"Idea 1: Multi-Response Batching","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Fit multiple Y vectors simultaneously Amortize setup costs GPU naturally parallel across responses Useful bootstrap, cross-validation","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"idea-2-approximate-path","dir":"","previous_headings":"Alternative Radical Ideas","what":"Idea 2: Approximate Path","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Don’t compute 100 lambdas Compute sparse grid (e.g., 10 values) Interpolate intermediate solutions Trade accuracy speed","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"idea-3-mixed-precision","dir":"","previous_headings":"Alternative Radical Ideas","what":"Idea 3: Mixed Precision","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Use float16 intermediate computations Keep float32 final results 2x memory bandwidth improvement Acceptable many applications","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"idea-4-neural-network-approximation","dir":"","previous_headings":"Alternative Radical Ideas","what":"Idea 4: Neural Network Approximation","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"Train small NN predict glmnet solution Use warm start iterative refinement Extremely fast inference Probably overkill, interesting","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/GLMNET_OPTIMIZATION_ANALYSIS.html","id":"conclusion","dir":"","previous_headings":"","what":"Conclusion","title":"mlxs_glmnet Performance Analysis and Optimization Strategies","text":"promising path forward Strategy 1 (Batched Coordinate Descent) : Matches glmnet’s proven algorithm Natural fit GPU parallelism Research supports feasibility Potential 10-100x speedup Combined Strategy 5 (improved strong rules) quick win. implementation : - Keep operations MLX final results - Process coordinates parallel batches - Handle multiple lambda values simultaneously - Use asynchronous updates convergence permits significant rewrite, necessary achieve competitive performance glmnet GPU hardware.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/articles/benchmarks.html","id":"data-generation","dir":"Articles","previous_headings":"","what":"Data Generation","title":"Benchmarks","text":"","code":"set.seed(20251111)  n_max <- 50000 p_max <- 200  X <- matrix(rnorm(n_max * p_max), nrow = n_max, ncol = p_max) colnames(X) <- paste0(\"x\", seq_len(p_max))  beta_true <- rnorm(p_max, mean = 0, sd = 0.5) y_continuous <- drop(X %*% beta_true + rnorm(n_max, sd = 2)) linpred <- drop(X %*% beta_true) / 5   prob <- 1 / (1 + exp(-linpred)) y_binary <- rbinom(n_max, size = 1, prob = prob)  full_data <- data.frame(   y_cont = y_continuous,   y_bin = y_binary,   X )  n_sizes <- c(2000, 10000, 50000) p_sizes <- c(25, 50, 100, 200)  # for fast debugging if (params$develop) {   p_sizes <- p_sizes/10   n_sizes <- n_sizes/10 }  bench_grid <- expand.grid(   n = n_sizes,   p = p_sizes,   stringsAsFactors = FALSE )  bench_grid <- bench_grid[bench_grid$n > bench_grid$p, ]"},{"path":"https://hughjonesd.github.io/RmlxStats/articles/benchmarks.html","id":"linear-model-benchmarks","dir":"Articles","previous_headings":"","what":"Linear Model Benchmarks","title":"Benchmarks","text":"","code":"lm_results <- list()  for (i in seq_len(nrow(bench_grid))) {   n <- bench_grid$n[i]   p <- bench_grid$p[i]    subset_data <- full_data[1:n, c(\"y_cont\", paste0(\"x\", 1:p))]   formula_str <- paste(\"y_cont ~\", paste(paste0(\"x\", 1:p), collapse = \" + \"))   lm_formula <- as.formula(formula_str)    bm <- mark(     \"stats::lm\" = lm(lm_formula, data = subset_data),     \"RmlxStats::mlxs_lm\" = {       l <- mlxs_lm(lm_formula, data = subset_data)       Rmlx::mlx_eval(l$coefficients)     },     \"fixest::feols\" = feols(lm_formula, data = subset_data),     \"RcppEigen::fastLm\" = RcppEigen::fastLm(lm_formula, data = subset_data),     \"speedglm::speedlm\" = speedglm::speedlm(lm_formula, data = subset_data),     iterations = 3,     check = FALSE,     filter_gc = FALSE   )    bm$n <- n   bm$p <- p   bm$model_type <- \"LM\"   lm_results[[i]] <- bm }  lm_df <- do.call(rbind, lm_results)"},{"path":"https://hughjonesd.github.io/RmlxStats/articles/benchmarks.html","id":"glm-benchmarks","dir":"Articles","previous_headings":"","what":"GLM Benchmarks","title":"Benchmarks","text":"","code":"glm_results <- list()  for (i in seq_len(nrow(bench_grid))) {   n <- bench_grid$n[i]   p <- bench_grid$p[i]    subset_data <- full_data[1:n, c(\"y_bin\", paste0(\"x\", 1:p))]   formula_str <- paste(\"y_bin ~\", paste(paste0(\"x\", 1:p), collapse = \" + \"))   glm_formula <- as.formula(formula_str)    bm <- mark(     \"stats::glm\" = glm(glm_formula, family = binomial(),                              data = subset_data,                              control = list(maxit = 50)),     \"RmlxStats::mlxs_glm\" = {       g <- mlxs_glm(glm_formula, family = mlxs_binomial(),                         data = subset_data,                         control = list(maxit = 50, epsilon = 1e-6))       Rmlx::mlx_eval(g$coefficients)     },     \"speedglm::speedglm\" = speedglm::speedglm(glm_formula, family = binomial(),                                    data = subset_data),     iterations = 3,     check = FALSE,     filter_gc = FALSE   )    bm$n <- n   bm$p <- p   bm$model_type <- \"GLM\"   glm_results[[i]] <- bm }  glm_df <- do.call(rbind, glm_results)"},{"path":"https://hughjonesd.github.io/RmlxStats/articles/benchmarks.html","id":"bootstrap-benchmarks","dir":"Articles","previous_headings":"","what":"Bootstrap Benchmarks","title":"Benchmarks","text":"bootstrap, use smaller datasets due computational cost.","code":"boot_grid <- expand.grid(   n = n_sizes[1:2],   p = p_sizes[1:3],   stringsAsFactors = FALSE )  boot_results <- list()  for (i in seq_len(nrow(boot_grid))) {   n <- boot_grid$n[i]   p <- boot_grid$p[i]    subset_data <- full_data[1:n, c(\"y_cont\", paste0(\"x\", 1:p))]   formula_str <- paste(\"y_cont ~\", paste(paste0(\"x\", 1:p), collapse = \" + \"))   boot_formula <- as.formula(formula_str)    fit_mlxs <- mlxs_lm(boot_formula, data = subset_data)   fit_base <- lm(boot_formula, data = subset_data)    # Bootstrap function for boot package   boot_stat <- function(dat, idx) {     coef(lm(boot_formula, data = dat[idx, , drop = FALSE]))   }    bm <- mark(     boot_case = boot::boot(subset_data, statistic = boot_stat,                           R = 50L, parallel = \"no\"),     lmboot_case = lmboot::paired.boot(boot_formula, data = subset_data,                                        B = 50L),     lmboot_resid = lmboot::residual.boot(boot_formula, data = subset_data,                                           B = 50L),     mlxs_case = {       s <- summary(fit_mlxs, bootstrap = TRUE,               bootstrap_args = list(B = 50L, seed = 42,                                    bootstrap_type = \"case\",                                    progress = FALSE))       Rmlx::mlx_eval(s$std.err)     },     mlxs_resid = {       s <- summary(fit_mlxs, bootstrap = TRUE,               bootstrap_args = list(B = 50L, seed = 42,                                    bootstrap_type = \"resid\",                                    progress = FALSE))       Rmlx::mlx_eval(s$std.err)     },     iterations = 3,     check = FALSE,     filter_gc = FALSE,     memory = FALSE   )    bm$n <- n   bm$p <- p   bm$model_type <- \"Bootstrap\"   boot_results[[i]] <- bm }  boot_df <- do.call(rbind, boot_results)"},{"path":"https://hughjonesd.github.io/RmlxStats/articles/benchmarks.html","id":"summary-tables","dir":"Articles","previous_headings":"","what":"Summary Tables","title":"Benchmarks","text":"compare RmlxStats functions base R, fastest alternative tested. Numbers show RmlxStats time/alternative time. RmlxStats time vs base R (%). 100% → RmlxStats faster RmlxStats time vs fastest alternative (%). 100% → RmlxStats faster","code":""},{"path":[]},{"path":[]},{"path":[]},{"path":"https://hughjonesd.github.io/RmlxStats/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"David Hugh-Jones. Author, maintainer. R Core Team. Contributor. Trevor Hastie. Contributor. Jerome Friedman. Contributor. Rob Tibshirani. Contributor. Balasubramanian Narasimhan. Contributor. Kenneth Tay. Contributor. Noah Simon. Contributor. James Yang. Contributor.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Hugh-Jones D (2025). RmlxStats: MLX-Accelerated Statistical Models. R package version 0.1.0, https://hughjonesd.github.io/RmlxStats.","code":"@Manual{,   title = {RmlxStats: MLX-Accelerated Statistical Models},   author = {David Hugh-Jones},   year = {2025},   note = {R package version 0.1.0},   url = {https://hughjonesd.github.io/RmlxStats}, }"},{"path":"https://hughjonesd.github.io/RmlxStats/index.html","id":"rmlxstats","dir":"","previous_headings":"","what":"MLX-Accelerated Statistical Models","title":"MLX-Accelerated Statistical Models","text":"Statistical modelling front-ends run Apple GPU hardware via Rmlx array library. GPUs designed handle matrices, good fit statistics. till now R Mac users access power GPUs. RmlxStats experiment implementing common statistical methods GPU. RmlxStats early work progress! Functions implemented far include Rmlx versions lm, glm, glmnet bootstrapping function mlxs_boot().","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/index.html","id":"when-to-use","dir":"","previous_headings":"","what":"When to use","title":"MLX-Accelerated Statistical Models","text":"RmlxStats offers large speedups base R functions, speed-optimized packages like speedglm RCppEigen. Speedups especially large regressions many predictors (large p). roughly, 50 predictors 10,000 rows, regressions taking measurable time complete, RmlxStats worth trying: See benchmarks vignette details. GPU calculations use float32 precision, need higher numerical accuracy , RmlxStats may right tool.","code":"# On my machine > system.time({lm <- lm(arr_delay ~ dep_delay + factor(paste(month,day)), data = nycflights13::flights); })    user  system elapsed   31.310   0.179  31.479   > system.time({lm2 <- mlxs_lm(arr_delay ~ dep_delay + factor(paste(month,day)), data = nycflights13::flights); Rmlx::mlx_eval(lm2$coefficients)})    user  system elapsed    4.421   0.271   2.818"},{"path":"https://hughjonesd.github.io/RmlxStats/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"MLX-Accelerated Statistical Models","text":"Install Apple’s MLX runtime: : also install Rmlx.","code":"brew install mlx remotes::install_github(\"hughjonesd/RmlxStats\")"},{"path":"https://hughjonesd.github.io/RmlxStats/reference/generics-reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Re-export generics — generics-reexports","title":"Re-export generics — generics-reexports","text":"generics re-exported generics package convenience.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_binomial.html","id":null,"dir":"Reference","previous_headings":"","what":"MLX-friendly binomial family — mlxs_binomial","title":"MLX-friendly binomial family — mlxs_binomial","text":"Construct binomial GLM family whose core link deviance helpers implemented R work MLX arrays well base R vectors. avoids calling compiled C routines handle base types.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_binomial.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MLX-friendly binomial family — mlxs_binomial","text":"","code":"mlxs_binomial(link = \"logit\")"},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_binomial.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MLX-friendly binomial family — mlxs_binomial","text":"link specification model link function.  can     name/expression, literal character string, length-one character     vector, object class     \"link-glm\" (generated     make.link) provided specified     via one standard names given next. gaussian family accepts links (names)     identity, log inverse;     binomial family links logit,     probit, cauchit, (corresponding logistic,     normal Cauchy CDFs respectively) log     cloglog (complementary log-log);     Gamma family links inverse, identity      log;     poisson family links log, identity,     sqrt; inverse.gaussian family links     1/mu^2, inverse, identity     log. quasi family accepts links logit, probit,     cloglog,  identity, inverse,     log, 1/mu^2 sqrt,     function power can used create     power link function.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_binomial.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"MLX-friendly binomial family — mlxs_binomial","text":"family object compatible stats::glm() mlxs_glm().","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_binomial.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"MLX-friendly binomial family — mlxs_binomial","text":"Currently logit, log, cloglog, cauchit links supported. link specifications, fall back stats::binomial().","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_boot.html","id":null,"dir":"Reference","previous_headings":"","what":"Bootstrap MLX arrays along the first dimension — mlxs_boot","title":"Bootstrap MLX arrays along the first dimension — mlxs_boot","text":"mlxs_boot() resamples observations one MLX arrays, calls user-supplied function resampled batch, returns collected results. Every argument supplied via ... must share size first dimension (number observations). Arguments need resampling captured environment fun instead passed ....","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_boot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bootstrap MLX arrays along the first dimension — mlxs_boot","text":"","code":"mlxs_boot(fun, ..., B = 200L, seed = NULL, progress = FALSE, compile = FALSE)"},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_boot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bootstrap MLX arrays along the first dimension — mlxs_boot","text":"fun Function called bootstrap draw. must accept named arguments supplied .... ... Arrays, matrices, vectors resampled along first dimension passed fun. B Number bootstrap iterations. seed Optional integer seed reproducibility. progress Logical; TRUE, show text progress bar. compile Logical; compile fun via Rmlx::mlx_compile() entering resampling loop. Defaults FALSE.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_boot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bootstrap MLX arrays along the first dimension — mlxs_boot","text":"list elements samples (raw results fun), B, seed.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_gaussian.html","id":null,"dir":"Reference","previous_headings":"","what":"MLX-friendly Gaussian family — mlxs_gaussian","title":"MLX-friendly Gaussian family — mlxs_gaussian","text":"MLX-friendly Gaussian family","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_gaussian.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MLX-friendly Gaussian family — mlxs_gaussian","text":"","code":"mlxs_gaussian(link = \"identity\")"},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_gaussian.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MLX-friendly Gaussian family — mlxs_gaussian","text":"link specification model link function.  can     name/expression, literal character string, length-one character     vector, object class     \"link-glm\" (generated     make.link) provided specified     via one standard names given next. gaussian family accepts links (names)     identity, log inverse;     binomial family links logit,     probit, cauchit, (corresponding logistic,     normal Cauchy CDFs respectively) log     cloglog (complementary log-log);     Gamma family links inverse, identity      log;     poisson family links log, identity,     sqrt; inverse.gaussian family links     1/mu^2, inverse, identity     log. quasi family accepts links logit, probit,     cloglog,  identity, inverse,     log, 1/mu^2 sqrt,     function power can used create     power link function.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_gaussian.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"MLX-friendly Gaussian family — mlxs_gaussian","text":"family object compatible mlxs_glm().","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"MLX-backed generalized linear model — mlxs_glm","title":"MLX-backed generalized linear model — mlxs_glm","text":"Fit generalized linear models using iterative reweighted least squares (IRLS) MLX providing heavy lifting weighted least squares solves.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MLX-backed generalized linear model — mlxs_glm","text":"","code":"mlxs_glm(   formula,   family = mlxs_gaussian(),   data,   subset,   weights,   na.action,   start = NULL,   control = list(),   ... )"},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_glm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MLX-backed generalized linear model — mlxs_glm","text":"formula object class \"formula\" (one     can coerced class): symbolic description     model fitted.  details model specification given     ‘Details’. family mlxs family object (e.g., mlxs_gaussian(), mlxs_binomial(), mlxs_poisson()). data optional data frame, list environment (object     coercible .data.frame data frame) containing     variables model.  found data,     variables taken environment(formula),     typically environment glm called. subset optional vector specifying subset observations     used fitting process. weights optional vector ‘prior weights’ used     fitting process.  NULL numeric vector. na.action function indicates happen     data contain NAs.  default set     na.action setting options,     na.fail unset.  ‘factory-fresh’     default na.omit.  Another possible value     NULL, action.  Value na.exclude can useful. start starting values parameters linear predictor. control Optional list control parameters passed stats::glm.control(). ... glm: arguments used form default     control argument supplied directly. weights: arguments passed methods.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_glm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"MLX-backed generalized linear model — mlxs_glm","text":"object class c(\"mlxs_glm\", \"mlxs_model\") containing elements similar result stats::glm(). Computations use single-precision MLX arrays, results typically agree stats::glm() around 1e-6 unless tighter tolerance supplied via control.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_glm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"MLX-backed generalized linear model — mlxs_glm","text":"","code":"fit <- mlxs_glm(mpg ~ cyl + disp, family = mlxs_gaussian(), data = mtcars) coef(fit) #> mlx array [3 x 1] #>   dtype: float32 #>   device: gpu #>   values: #>             [,1] #> [1,] 34.66099167 #> [2,] -1.58727658 #> [3,] -0.02058364 #> attr(,\"coef_names\") #> [1] \"(Intercept)\" \"cyl\"         \"disp\""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_glm_methods.html","id":null,"dir":"Reference","previous_headings":"","what":"mlxs_glm method utilities — mlxs_glm_methods","title":"mlxs_glm method utilities — mlxs_glm_methods","text":"Support functions provide familiar S3 surface mlxs_glm fits delegating equivalent base glm behaviour helpful.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_glm_methods.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"mlxs_glm method utilities — mlxs_glm_methods","text":"","code":"# S3 method for class 'mlxs_glm' coef(object, ...)  # S3 method for class 'mlxs_glm' predict(   object,   newdata = NULL,   type = c(\"link\", \"response\"),   se.fit = FALSE,   ... )  # S3 method for class 'mlxs_glm' fitted(object, ...)  # S3 method for class 'mlxs_glm' residuals(object, type = c(\"deviance\", \"pearson\", \"working\", \"response\"), ...)  # S3 method for class 'mlxs_glm' vcov(object, ...)  # S3 method for class 'mlxs_glm' print(x, digits = max(3, getOption(\"digits\") - 3), ...)  # S3 method for class 'mlxs_glm' summary(object, bootstrap = FALSE, bootstrap_args = list(), ...)  # S3 method for class 'summary.mlxs_glm' print(x, digits = max(3, getOption(\"digits\") - 3), ...)  # S3 method for class 'mlxs_glm' anova(object, ...)  # S3 method for class 'mlxs_glm' model.frame(formula, ...)  # S3 method for class 'mlxs_glm' model.matrix(object, ...)  # S3 method for class 'mlxs_glm' terms(x, ...)  # S3 method for class 'mlxs_glm' nobs(object, ...)  # S3 method for class 'mlxs_glm' tidy(x, ...)  # S3 method for class 'mlxs_glm' glance(x, ...)  # S3 method for class 'mlxs_glm' augment(   x,   data = x$model,   newdata = NULL,   type.predict = c(\"response\", \"link\"),   type.residuals = c(\"response\", \"deviance\"),   se_fit = FALSE,   output = c(\"data.frame\", \"mlx\"),   ... )"},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_glm_methods.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"mlxs_glm method utilities — mlxs_glm_methods","text":"object mlxs_glm model fit. ... Additional arguments passed underlying methods. newdata Optional data frame used prediction. type Character string indicating scale prediction residuals return. se.fit Logical. standard errors fit returned supported? x mlxs_glm model fit (methods leading x argument). digits Number significant digits print summaries. bootstrap Logical; bootstrap standard errors computed? bootstrap_args List bootstrap configuration options. formula, data Optional formula data overrides used augment.mlxs_glm(). type.predict, type.residuals Character strings controlling scale fitted values residuals returned augment.mlxs_glm(). se_fit Logical; standard-error analogue augment. output Character string; return format (\"data.frame\" \"mlx\").","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_glmnet.html","id":null,"dir":"Reference","previous_headings":"","what":"MLX-backed elastic net regression — mlxs_glmnet","title":"MLX-backed elastic net regression — mlxs_glmnet","text":"Fit lasso elastic-net penalised regression paths using MLX tensors heavy linear algebra. Currently supports Gaussian binomial families optional intercept column standardisation.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_glmnet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MLX-backed elastic net regression — mlxs_glmnet","text":"","code":"mlxs_glmnet(   x,   y,   family = mlxs_gaussian(),   alpha = 1,   lambda = NULL,   nlambda = 100,   lambda_min_ratio = 1e-04,   standardize = TRUE,   intercept = TRUE,   maxit = 1000,   tol = 1e-06 )"},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_glmnet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MLX-backed elastic net regression — mlxs_glmnet","text":"x Numeric matrix predictors (observations rows). y Numeric response vector (binomial, values must 0/1). family MLX-aware family object, e.g. mlxs_gaussian() mlxs_binomial(). alpha Elastic-net mixing parameter (1 = lasso, currently alpha must strictly positive). lambda Optional decreasing sequence penalty values. NULL, sequence length nlambda generated lambda_max lambda_max * lambda_min_ratio. nlambda Length automatically generated lambda path. lambda_min_ratio Smallest lambda fraction lambda_max. standardize columns x centred scaled fitting? Coefficients returned original scale regardless. intercept intercept fit? maxit Maximum proximal-gradient iterations per lambda value. tol Convergence tolerance coefficient updates.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_glmnet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"MLX-backed elastic net regression — mlxs_glmnet","text":"object class mlxs_glmnet containing fitted coefficient path, intercepts, lambda sequence, scaling information.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_glmnet.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"MLX-backed elastic net regression — mlxs_glmnet","text":"function proof--concept. large dense problems typically several times slower highly optimised glmnet::glmnet() implementation.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_lm.html","id":null,"dir":"Reference","previous_headings":"","what":"MLX-backed linear regression — mlxs_lm","title":"MLX-backed linear regression — mlxs_lm","text":"Fit linear model via QR decomposition using MLX arrays Apple Silicon devices. interface mirrors stats::lm() common arguments.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_lm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MLX-backed linear regression — mlxs_lm","text":"","code":"mlxs_lm(formula, data, subset, weights)"},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_lm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MLX-backed linear regression — mlxs_lm","text":"formula Model formula. data Optional data frame, tibble, environment containing variables model. subset Optional expression subsetting observations. weights Optional non-negative observation weights. Treated like weights argument stats::lm(), .e. enter fit via weighted least squares.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_lm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"MLX-backed linear regression — mlxs_lm","text":"object class c(\"mlxs_lm\", \"mlxs_model\") containing components similar \"lm\" fit, along MLX intermediates stored mlx element. Note MLX currently operates single precision, fitted values diagnostics may differ stats::lm() around 1e-6 level.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_lm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"MLX-backed linear regression — mlxs_lm","text":"","code":"fit <- mlxs_lm(mpg ~ cyl + disp, data = mtcars) coef(fit) #> mlx array [3 x 1] #>   dtype: float32 #>   device: gpu #>   values: #>             [,1] #> [1,] 34.66099167 #> [2,] -1.58727658 #> [3,] -0.02058364 #> attr(,\"coef_names\") #> [1] \"(Intercept)\" \"cyl\"         \"disp\""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_lm_methods.html","id":null,"dir":"Reference","previous_headings":"","what":"mlxs_lm method utilities — mlxs_lm_methods","title":"mlxs_lm method utilities — mlxs_lm_methods","text":"helpers provide familiar S3 surface mlxs_lm fits.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_lm_methods.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"mlxs_lm method utilities — mlxs_lm_methods","text":"object mlxs_lm model fit. x mlxs_lm model fit (methods leading x argument). ... Additional arguments passed underlying methods. newdata Optional data frame prediction. parm Parameter specification confidence intervals. level Confidence level intervals. bootstrap Logical; bootstrap standard errors computed? bootstrap_args List bootstrap configuration options. evaluate Logical; evaluate updated call? formula mlxs_lm object used place formula model.frame. data Optional data frame augment. se_fit Logical; standard errors fit included? output Character string; return format (\"data.frame\" \"mlx\"). row.names Optional row names data frame conversion. optional Logical; passed .data.frame. digits Number significant digits printing.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_poisson.html","id":null,"dir":"Reference","previous_headings":"","what":"MLX-friendly Poisson family — mlxs_poisson","title":"MLX-friendly Poisson family — mlxs_poisson","text":"MLX-friendly Poisson family","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_poisson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MLX-friendly Poisson family — mlxs_poisson","text":"","code":"mlxs_poisson(link = \"log\")"},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_poisson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MLX-friendly Poisson family — mlxs_poisson","text":"link specification model link function.  can     name/expression, literal character string, length-one character     vector, object class     \"link-glm\" (generated     make.link) provided specified     via one standard names given next. gaussian family accepts links (names)     identity, log inverse;     binomial family links logit,     probit, cauchit, (corresponding logistic,     normal Cauchy CDFs respectively) log     cloglog (complementary log-log);     Gamma family links inverse, identity      log;     poisson family links log, identity,     sqrt; inverse.gaussian family links     1/mu^2, inverse, identity     log. quasi family accepts links logit, probit,     cloglog,  identity, inverse,     log, 1/mu^2 sqrt,     function power can used create     power link function.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_poisson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"MLX-friendly Poisson family — mlxs_poisson","text":"family object compatible mlxs_glm().","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_quasibinomial.html","id":null,"dir":"Reference","previous_headings":"","what":"MLX-friendly quasibinomial family — mlxs_quasibinomial","title":"MLX-friendly quasibinomial family — mlxs_quasibinomial","text":"MLX-friendly quasibinomial family","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_quasibinomial.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MLX-friendly quasibinomial family — mlxs_quasibinomial","text":"","code":"mlxs_quasibinomial(link = \"logit\")"},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_quasibinomial.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MLX-friendly quasibinomial family — mlxs_quasibinomial","text":"link specification model link function.  can     name/expression, literal character string, length-one character     vector, object class     \"link-glm\" (generated     make.link) provided specified     via one standard names given next. gaussian family accepts links (names)     identity, log inverse;     binomial family links logit,     probit, cauchit, (corresponding logistic,     normal Cauchy CDFs respectively) log     cloglog (complementary log-log);     Gamma family links inverse, identity      log;     poisson family links log, identity,     sqrt; inverse.gaussian family links     1/mu^2, inverse, identity     log. quasi family accepts links logit, probit,     cloglog,  identity, inverse,     log, 1/mu^2 sqrt,     function power can used create     power link function.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_quasibinomial.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"MLX-friendly quasibinomial family — mlxs_quasibinomial","text":"family object compatible mlxs_glm().","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_quasipoisson.html","id":null,"dir":"Reference","previous_headings":"","what":"MLX-friendly quasipoisson family — mlxs_quasipoisson","title":"MLX-friendly quasipoisson family — mlxs_quasipoisson","text":"MLX-friendly quasipoisson family","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_quasipoisson.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MLX-friendly quasipoisson family — mlxs_quasipoisson","text":"","code":"mlxs_quasipoisson(link = \"log\")"},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_quasipoisson.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MLX-friendly quasipoisson family — mlxs_quasipoisson","text":"link specification model link function.  can     name/expression, literal character string, length-one character     vector, object class     \"link-glm\" (generated     make.link) provided specified     via one standard names given next. gaussian family accepts links (names)     identity, log inverse;     binomial family links logit,     probit, cauchit, (corresponding logistic,     normal Cauchy CDFs respectively) log     cloglog (complementary log-log);     Gamma family links inverse, identity      log;     poisson family links log, identity,     sqrt; inverse.gaussian family links     1/mu^2, inverse, identity     log. quasi family accepts links logit, probit,     cloglog,  identity, inverse,     log, 1/mu^2 sqrt,     function power can used create     power link function.","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/mlxs_quasipoisson.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"MLX-friendly quasipoisson family — mlxs_quasipoisson","text":"family object compatible mlxs_glm().","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/reference/reexports.html","id":null,"dir":"Reference","previous_headings":"","what":"Objects exported from other packages — reexports","title":"Objects exported from other packages — reexports","text":"objects imported packages. Follow links see documentation. generics augment, glance, tidy","code":""},{"path":"https://hughjonesd.github.io/RmlxStats/news/index.html","id":"rmlxstats-010","dir":"Changelog","previous_headings":"","what":"RmlxStats 0.1.0","title":"RmlxStats 0.1.0","text":"Initial version.","code":""}]
